.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_advanced_run_hyperparameter_tunning.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_advanced_run_hyperparameter_tunning.py:


Tunning Hyperparameters
=======================

This example uses the 'fmri' dataset, performs simple binary classification
using a Support Vector Machine classifier and analyse the model.


References
----------
Waskom, M.L., Frank, M.C., Wagner, A.D. (2016). Adaptive engagement of
cognitive control in context-dependent decision-making. Cerebral Cortex.


.. include:: ../../links.inc


.. code-block:: default

    # Authors: Federico Raimondo <f.raimondo@fz-juelich.de>
    #
    # License: AGPL
    import numpy as np
    from seaborn import load_dataset

    from julearn import run_cross_validation
    from julearn.utils import configure_logging








Set the logging level to info to see extra information


.. code-block:: default

    configure_logging(level='INFO')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-11-10 11:48:12,300 - julearn - INFO - ===== Lib Versions =====
    2020-11-10 11:48:12,301 - julearn - INFO - numpy: 1.19.0
    2020-11-10 11:48:12,301 - julearn - INFO - scipy: 1.5.4
    2020-11-10 11:48:12,301 - julearn - INFO - sklearn: 0.23.2
    2020-11-10 11:48:12,301 - julearn - INFO - pandas: 1.1.4
    2020-11-10 11:48:12,301 - julearn - INFO - julearn: 0.1.0
    2020-11-10 11:48:12,301 - julearn - INFO - ========================




Set the random seed to always have the same example


.. code-block:: default

    np.random.seed(42)









Load the dataset


.. code-block:: default

    df_fmri = load_dataset('fmri')
    print(df_fmri.head())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      subject  timepoint event    region    signal
    0     s13         18  stim  parietal -0.017552
    1      s5         14  stim  parietal -0.080883
    2     s12         18  stim  parietal -0.081033
    3     s11         18  stim  parietal -0.046134
    4     s10         18  stim  parietal -0.037970




Set the dataframe in the right format


.. code-block:: default

    df_fmri = df_fmri.pivot(
        index=['subject', 'timepoint', 'event'],
        columns='region',
        values='signal')

    df_fmri = df_fmri.reset_index()
    print(df_fmri.head())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    region subject  timepoint event   frontal  parietal
    0           s0          0   cue  0.007766 -0.006899
    1           s0          0  stim -0.021452 -0.039327
    2           s0          1   cue  0.016440  0.000300
    3           s0          1  stim -0.021054 -0.035735
    4           s0          2   cue  0.024296  0.033220




Lets do a first attempt and use a linear SVM with the default parameters.


.. code-block:: default

    model_params = {'svm__kernel': 'linear'}
    X = ['frontal', 'parietal']
    y = 'event'
    scores = run_cross_validation(X=X, y=y, data=df_fmri, model='svm',
                                  model_params=model_params)

    print(scores['test_score'].mean())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-11-10 11:48:12,327 - julearn - INFO - Using default CV
    2020-11-10 11:48:12,327 - julearn - INFO - ==== Input Data ====
    2020-11-10 11:48:12,327 - julearn - INFO - Using dataframe as input
    2020-11-10 11:48:12,327 - julearn - INFO - Features: ['frontal', 'parietal']
    2020-11-10 11:48:12,327 - julearn - INFO - Target: event
    2020-11-10 11:48:12,327 - julearn - INFO - Expanded X: ['frontal', 'parietal']
    2020-11-10 11:48:12,327 - julearn - INFO - Expanded Confounds: []
    2020-11-10 11:48:12,328 - julearn - INFO - ====================
    2020-11-10 11:48:12,329 - julearn - INFO - 
    2020-11-10 11:48:12,329 - julearn - INFO - ====== Model ======
    2020-11-10 11:48:12,329 - julearn - INFO - Obtaining model by name: svm
    2020-11-10 11:48:12,329 - julearn - INFO - ===================
    2020-11-10 11:48:12,329 - julearn - INFO - 
    2020-11-10 11:48:12,329 - julearn - INFO - CV interpeted as RepeatedKFold with 5 repetitions of 5 folds
    2020-11-10 11:48:12,329 - julearn - INFO - = Model Parameters =
    2020-11-10 11:48:12,330 - julearn - INFO - ====================
    2020-11-10 11:48:12,330 - julearn - INFO - 
    0.5765508728619291




The score is not so good. Lets try to see if there is an optimal
regularization parameter (C) for the linear SVM.


.. code-block:: default

    model_params = {
        'svm__kernel': 'linear',
        'svm__C': [0.01, 0.1],
        'cv': 2}  # CV=2 too speed up the example
    X = ['frontal', 'parietal']
    y = 'event'
    scores, estimator = run_cross_validation(X=X, y=y, data=df_fmri, model='svm',
                                             model_params=model_params,
                                             return_estimator=True)

    print(scores['test_score'].mean())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-11-10 11:48:12,819 - julearn - INFO - Using default CV
    2020-11-10 11:48:12,819 - julearn - INFO - ==== Input Data ====
    2020-11-10 11:48:12,819 - julearn - INFO - Using dataframe as input
    2020-11-10 11:48:12,819 - julearn - INFO - Features: ['frontal', 'parietal']
    2020-11-10 11:48:12,819 - julearn - INFO - Target: event
    2020-11-10 11:48:12,819 - julearn - INFO - Expanded X: ['frontal', 'parietal']
    2020-11-10 11:48:12,819 - julearn - INFO - Expanded Confounds: []
    2020-11-10 11:48:12,820 - julearn - INFO - ====================
    2020-11-10 11:48:12,820 - julearn - INFO - 
    2020-11-10 11:48:12,821 - julearn - INFO - ====== Model ======
    2020-11-10 11:48:12,821 - julearn - INFO - Obtaining model by name: svm
    2020-11-10 11:48:12,821 - julearn - INFO - ===================
    2020-11-10 11:48:12,821 - julearn - INFO - 
    2020-11-10 11:48:12,821 - julearn - INFO - CV interpeted as RepeatedKFold with 5 repetitions of 5 folds
    2020-11-10 11:48:12,821 - julearn - INFO - = Model Parameters =
    2020-11-10 11:48:12,822 - julearn - INFO - Tunning hyperparameters using {search}
    2020-11-10 11:48:12,822 - julearn - INFO - Hyperparameters:
    2020-11-10 11:48:12,822 - julearn - INFO -      dataframe_pipeline__svm__C: [0.01, 0.1]
    2020-11-10 11:48:12,823 - julearn - INFO - Using scikit-learn CV scheme KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:12,823 - julearn - INFO - Search Parameters:
    2020-11-10 11:48:12,823 - julearn - INFO -      cv: KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:12,823 - julearn - INFO -      scoring: None
    2020-11-10 11:48:12,823 - julearn - INFO - ====================
    2020-11-10 11:48:12,823 - julearn - INFO - 
    0.575591606418621




This did not change much, lets explore other kernels too.


.. code-block:: default

    model_params = {
        'svm__kernel': ['linear', 'rbf', 'poly'],
        'svm__C': [0.01, 0.1],
        'cv': 2}  # CV=2 too speed up the example
    X = ['frontal', 'parietal']
    y = 'event'
    scores, estimator = run_cross_validation(X=X, y=y, data=df_fmri, model='svm',
                                             model_params=model_params,
                                             return_estimator=True)

    print(scores['test_score'].mean())




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-11-10 11:48:15,360 - julearn - INFO - Using default CV
    2020-11-10 11:48:15,360 - julearn - INFO - ==== Input Data ====
    2020-11-10 11:48:15,360 - julearn - INFO - Using dataframe as input
    2020-11-10 11:48:15,360 - julearn - INFO - Features: ['frontal', 'parietal']
    2020-11-10 11:48:15,361 - julearn - INFO - Target: event
    2020-11-10 11:48:15,361 - julearn - INFO - Expanded X: ['frontal', 'parietal']
    2020-11-10 11:48:15,361 - julearn - INFO - Expanded Confounds: []
    2020-11-10 11:48:15,362 - julearn - INFO - ====================
    2020-11-10 11:48:15,362 - julearn - INFO - 
    2020-11-10 11:48:15,362 - julearn - INFO - ====== Model ======
    2020-11-10 11:48:15,362 - julearn - INFO - Obtaining model by name: svm
    2020-11-10 11:48:15,362 - julearn - INFO - ===================
    2020-11-10 11:48:15,362 - julearn - INFO - 
    2020-11-10 11:48:15,362 - julearn - INFO - CV interpeted as RepeatedKFold with 5 repetitions of 5 folds
    2020-11-10 11:48:15,363 - julearn - INFO - = Model Parameters =
    2020-11-10 11:48:15,363 - julearn - INFO - Tunning hyperparameters using {search}
    2020-11-10 11:48:15,363 - julearn - INFO - Hyperparameters:
    2020-11-10 11:48:15,363 - julearn - INFO -      dataframe_pipeline__svm__kernel: ['linear', 'rbf', 'poly']
    2020-11-10 11:48:15,363 - julearn - INFO -      dataframe_pipeline__svm__C: [0.01, 0.1]
    2020-11-10 11:48:15,363 - julearn - INFO - Using scikit-learn CV scheme KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:15,363 - julearn - INFO - Search Parameters:
    2020-11-10 11:48:15,363 - julearn - INFO -      cv: KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:15,363 - julearn - INFO -      scoring: None
    2020-11-10 11:48:15,364 - julearn - INFO - ====================
    2020-11-10 11:48:15,364 - julearn - INFO - 
    0.7116487391994357




It seems that we might have found a better model, but which one is it?


.. code-block:: default

    print(estimator.best_params_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    {'dataframe_pipeline__svm__C': 0.1, 'dataframe_pipeline__svm__kernel': 'rbf'}




Now that we know that a RBF kernel is better, lest test different *gamma*
parameters.


.. code-block:: default

    model_params = {
        'svm__kernel': 'rbf',
        'svm__C': [0.01, 0.1],
        'svm__gamma': [1e-2, 1e-3],
        'cv': 2}  # CV=2 too speed up the example
    X = ['frontal', 'parietal']
    y = 'event'
    scores, estimator = run_cross_validation(X=X, y=y, data=df_fmri, model='svm',
                                             model_params=model_params,
                                             return_estimator=True)

    print(scores['test_score'].mean())
    print(estimator.best_params_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-11-10 11:48:21,929 - julearn - INFO - Using default CV
    2020-11-10 11:48:21,929 - julearn - INFO - ==== Input Data ====
    2020-11-10 11:48:21,929 - julearn - INFO - Using dataframe as input
    2020-11-10 11:48:21,930 - julearn - INFO - Features: ['frontal', 'parietal']
    2020-11-10 11:48:21,930 - julearn - INFO - Target: event
    2020-11-10 11:48:21,930 - julearn - INFO - Expanded X: ['frontal', 'parietal']
    2020-11-10 11:48:21,930 - julearn - INFO - Expanded Confounds: []
    2020-11-10 11:48:21,931 - julearn - INFO - ====================
    2020-11-10 11:48:21,931 - julearn - INFO - 
    2020-11-10 11:48:21,931 - julearn - INFO - ====== Model ======
    2020-11-10 11:48:21,931 - julearn - INFO - Obtaining model by name: svm
    2020-11-10 11:48:21,931 - julearn - INFO - ===================
    2020-11-10 11:48:21,932 - julearn - INFO - 
    2020-11-10 11:48:21,932 - julearn - INFO - CV interpeted as RepeatedKFold with 5 repetitions of 5 folds
    2020-11-10 11:48:21,932 - julearn - INFO - = Model Parameters =
    2020-11-10 11:48:21,933 - julearn - INFO - Tunning hyperparameters using {search}
    2020-11-10 11:48:21,933 - julearn - INFO - Hyperparameters:
    2020-11-10 11:48:21,933 - julearn - INFO -      dataframe_pipeline__svm__C: [0.01, 0.1]
    2020-11-10 11:48:21,933 - julearn - INFO -      dataframe_pipeline__svm__gamma: [0.01, 0.001]
    2020-11-10 11:48:21,933 - julearn - INFO - Using scikit-learn CV scheme KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:21,933 - julearn - INFO - Search Parameters:
    2020-11-10 11:48:21,933 - julearn - INFO -      cv: KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:21,933 - julearn - INFO -      scoring: None
    2020-11-10 11:48:21,933 - julearn - INFO - ====================
    2020-11-10 11:48:21,934 - julearn - INFO - 
    0.47479104214424267
    {'dataframe_pipeline__svm__C': 0.01, 'dataframe_pipeline__svm__gamma': 0.001}




It seems that without tunning the gamma parameter we had a better accuracy.
Let's add the default value and see what happens.


.. code-block:: default

    model_params = {
        'svm__kernel': 'rbf',
        'svm__C': [0.01, 0.1],
        'svm__gamma': [1e-2, 1e-3, 'scale'],
        'cv': 2}  # CV=2 too speed up the example
    X = ['frontal', 'parietal']
    y = 'event'
    scores, estimator = run_cross_validation(X=X, y=y, data=df_fmri, model='svm',
                                             model_params=model_params,
                                             return_estimator=True)

    print(scores['test_score'].mean())
    print(estimator.best_params_)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-11-10 11:48:26,842 - julearn - INFO - Using default CV
    2020-11-10 11:48:26,842 - julearn - INFO - ==== Input Data ====
    2020-11-10 11:48:26,842 - julearn - INFO - Using dataframe as input
    2020-11-10 11:48:26,842 - julearn - INFO - Features: ['frontal', 'parietal']
    2020-11-10 11:48:26,842 - julearn - INFO - Target: event
    2020-11-10 11:48:26,842 - julearn - INFO - Expanded X: ['frontal', 'parietal']
    2020-11-10 11:48:26,843 - julearn - INFO - Expanded Confounds: []
    2020-11-10 11:48:26,844 - julearn - INFO - ====================
    2020-11-10 11:48:26,844 - julearn - INFO - 
    2020-11-10 11:48:26,844 - julearn - INFO - ====== Model ======
    2020-11-10 11:48:26,844 - julearn - INFO - Obtaining model by name: svm
    2020-11-10 11:48:26,844 - julearn - INFO - ===================
    2020-11-10 11:48:26,844 - julearn - INFO - 
    2020-11-10 11:48:26,845 - julearn - INFO - CV interpeted as RepeatedKFold with 5 repetitions of 5 folds
    2020-11-10 11:48:26,845 - julearn - INFO - = Model Parameters =
    2020-11-10 11:48:26,846 - julearn - INFO - Tunning hyperparameters using {search}
    2020-11-10 11:48:26,846 - julearn - INFO - Hyperparameters:
    2020-11-10 11:48:26,846 - julearn - INFO -      dataframe_pipeline__svm__C: [0.01, 0.1]
    2020-11-10 11:48:26,846 - julearn - INFO -      dataframe_pipeline__svm__gamma: [0.01, 0.001, 'scale']
    2020-11-10 11:48:26,846 - julearn - INFO - Using scikit-learn CV scheme KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:26,846 - julearn - INFO - Search Parameters:
    2020-11-10 11:48:26,846 - julearn - INFO -      cv: KFold(n_splits=2, random_state=None, shuffle=False)
    2020-11-10 11:48:26,846 - julearn - INFO -      scoring: None
    2020-11-10 11:48:26,847 - julearn - INFO - ====================
    2020-11-10 11:48:26,847 - julearn - INFO - 
    0.7074977958032092
    {'dataframe_pipeline__svm__C': 0.1, 'dataframe_pipeline__svm__gamma': 'scale'}




So what was the best ``gamma`` in the end?


.. code-block:: default

    print(estimator.best_estimator_['svm']._gamma)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.5





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  21.531 seconds)


.. _sphx_glr_download_auto_examples_advanced_run_hyperparameter_tunning.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_hyperparameter_tunning.py <run_hyperparameter_tunning.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_hyperparameter_tunning.ipynb <run_hyperparameter_tunning.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_advanced_run_advanced_confound_removal.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_advanced_run_advanced_confound_removal.py:


Confound Removal in Advanced Settings
=====================================

In most cases confound removal is a simple operation.
You regress out the confound from the features and only continue working with
these new deconfounded features. This example will not focus on these
typical cases. Instead, this will show how to keep the confounds and use
them as additional features.

.. include:: ../../links.inc


.. code-block:: default

    # Authors: Sami Hamdan <s.hamdan@fz-juelich.de>
    #
    # License: AGPL
    from sklearn.datasets import load_diabetes  # to load data
    from julearn.pipeline import create_extended_pipeline
    from julearn.transformers import (get_transformer,
                                      DataFrameConfoundRemover,
                                      ChangeColumnTypes)
    from julearn.estimators import get_model


    # load in the data
    df_features, target = load_diabetes(return_X_y=True, as_frame=True)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/travis/virtualenv/python3.6.7/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
      return f(*args, **kwds)




First, we can have a look at our features.
You can see it includes
Age, BMI, average blood pressure (bp) and 6 other measures from s1 to s6
Furthermore, it includes sex which will be considered as a confound in
this example.



.. code-block:: default

    print('Features: ', df_features.head())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Features:          age       sex       bmi  ...        s4        s5        s6
    0  0.038076  0.050680  0.061696  ... -0.002592  0.019908 -0.017646
    1 -0.001882 -0.044642 -0.051474  ... -0.039493 -0.068330 -0.092204
    2  0.085299  0.050680  0.044451  ... -0.002592  0.002864 -0.025930
    3 -0.089063 -0.044642 -0.011595  ...  0.034309  0.022692 -0.009362
    4  0.005383 -0.044642 -0.036385  ... -0.002592 -0.031991 -0.046641

    [5 rows x 10 columns]




Second, we can have a look at the target


.. code-block:: default

    print('Target: ', target.describe())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Target:  count    442.000000
    mean     152.133484
    std       77.093005
    min       25.000000
    25%       87.000000
    50%      140.500000
    75%      211.500000
    max      346.000000
    Name: target, dtype: float64




In the following we will explore different settings of confound removal
using Julearns pipeline functionalities.

.. note::
 Everything, shown here is also possible in Julearns `run_cross_validation`
 function.

Confound Removal Typical Use Case
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Here, we first deconfound the features.
The confound is automatically dropped by the `.DataFrameConfoundRemover`.
Afterwards, we will transform our features with a pca and run
a lienar regression



.. code-block:: default

    typical_pipe = create_extended_pipeline(
        preprocess_steps_features=[
            ('remove_confound', DataFrameConfoundRemover(), 'subset', 'all'),
            ('pca', *get_transformer('pca'))
        ],
        preprocess_steps_confounds=None,
        preprocess_transformer_target=None,
        model=('lr', get_model('linreg', 'regression')),
        confounds=['sex'],
        categorical_features=None,
    )
    typical_pipe.fit(df_features, target)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ExtendedDataFramePipeline(categorical_features=[], confounds=['sex'],
                              dataframe_pipeline=Pipeline(steps=[('remove_confound',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd3a128>),
                                                                 ('pca',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd3a080>),
                                                                 ['lr',
                                                                  LinearRegression()]]))



We can use the `preprocess` method of the `.ExtendedDataFramePipeline`
to inspect the transformations/preprocessing steps of our pipeline.
By providing a step name to the `until` argument of the
`preprocess` method we return the transformed X and y including to this step.
This output always includes the transformed
X and transformed y as a tuple.


.. code-block:: default

    X_deconfounded, _ = typical_pipe.preprocess(
        df_features, target, until='remove_confound')
    print(X_deconfounded.head())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

            age       bmi        bp  ...        s4        s5        s6
    0  0.029271  0.057228  0.009658  ... -0.019424  0.012311 -0.028194
    1  0.005874 -0.047538 -0.015569  ... -0.024667 -0.061637 -0.082913
    2  0.076494  0.039983 -0.017885  ... -0.019424 -0.004734 -0.036479
    3 -0.081307 -0.007659 -0.025897  ...  0.049135  0.029385 -0.000071
    4  0.013139 -0.032449  0.032632  ...  0.012234 -0.025299 -0.037349

    [5 rows x 9 columns]




As you can see the confound `sex` was dropped and only the deconfounded
features are used in the following pca.
But what if you want to keep the confound in the feature space.

For example, let's assume that you want to do a pca on the deconfounded
features, but still want to keep the confound as a feature.
This would mean that the following pca would also use the confound
as a feature.
Let us have a closer look to the confound remover in order to understand
how we could achieve such a task:

.. autoclass:: julearn.transformers.DataFrameConfoundRemover

As you can see above we can set the `keep_confounds` argument to True,
if we want to keep the confounds.
Here is an example of how this can look like:


.. code-block:: default


    keep_confound_pipe = create_extended_pipeline(
        preprocess_steps_features=[
            ('remove_confound', DataFrameConfoundRemover(
                keep_confounds=True), 'subset', 'all'),
            ('pca', *get_transformer('pca'))
        ],
        preprocess_steps_confounds=None,
        preprocess_transformer_target=None,
        model=('lr', get_model('linreg', 'regression')),
        confounds=['sex'],
        categorical_features=None,
    )
    keep_confound_pipe.fit(df_features, target)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ExtendedDataFramePipeline(categorical_features=[], confounds=['sex'],
                              dataframe_pipeline=Pipeline(steps=[('remove_confound',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd81208>),
                                                                 ('pca',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd81278>),
                                                                 ['lr',
                                                                  LinearRegression()]]))



As you can see this will keep the confound


.. code-block:: default

    X_deconfounded, _ = keep_confound_pipe.preprocess(
        df_features, target, until='remove_confound')
    print(X_deconfounded.head())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

            age       sex       bmi  ...        s4        s5        s6
    0  0.029271  0.050680  0.057228  ... -0.019424  0.012311 -0.028194
    1  0.005874 -0.044642 -0.047538  ... -0.024667 -0.061637 -0.082913
    2  0.076494  0.050680  0.039983  ... -0.019424 -0.004734 -0.036479
    3 -0.081307 -0.044642 -0.007659  ...  0.049135  0.029385 -0.000071
    4  0.013139 -0.044642 -0.032449  ...  0.012234 -0.025299 -0.037349

    [5 rows x 10 columns]




Even after the pca the confound will still be present
This is the case because by default transformers only transform continuous
features (including features without a specified type)
and the confound is of type confound.


.. code-block:: default

    X_transformed, _ = keep_confound_pipe.preprocess(df_features, target)
    print(X_transformed.head())

    # This means that the resulting Linear Regression will use the deconfounded
    # features together with the confound to predict the target.





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

       pca_component:0  pca_component:1  ...  pca_component:8       sex
    0        -0.014050         0.075715  ...        -0.002330  0.050680
    1        -0.099883        -0.062830  ...         0.002074 -0.044642
    2        -0.029014         0.053253  ...        -0.002579  0.050680
    3         0.035164        -0.001321  ...        -0.003546 -0.044642
    4        -0.003952        -0.025446  ...        -0.000516 -0.044642

    [5 rows x 10 columns]




Lastly, you can also use the confound as a normal feature after confound
removal. To do so you can either add the confound(s) to the
transformed_columns of the each following transformers
which return the same columns or you can use the
`.ChangeColumnTypes` to change the returned confounds
to a continuous variable like this:


.. code-block:: default

    confound_as_feature_pipe = create_extended_pipeline(
        preprocess_steps_features=[
            ('remove_confound', DataFrameConfoundRemover(
                keep_confounds=True), 'subset', 'all'),
            ('change_column_types', ChangeColumnTypes('.*confound', 'continuous'),
             'from_transformer', 'all'),
            ('pca', *get_transformer('pca'))
        ],
        preprocess_steps_confounds=None,
        preprocess_transformer_target=None,
        model=('lr', get_model('linreg', 'regression')),
        confounds=['sex'],
        categorical_features=None,
    )
    confound_as_feature_pipe.fit(df_features, target)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ExtendedDataFramePipeline(categorical_features=[], confounds=['sex'],
                              dataframe_pipeline=Pipeline(steps=[('remove_confound',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd81e48>),
                                                                 ('change_column_types',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd81fd0>),
                                                                 ('pca',
                                                                  <julearn.transformers.dataframe.DataFrameTransformer object at 0x7effcfd811d0>),
                                                                 ['lr',
                                                                  LinearRegression()]]))



As you can see this will keep the confound and
change its type to a continuous variable.


.. code-block:: default

    X_deconfounded, _ = confound_as_feature_pipe.preprocess(
        df_features, target, until='change_column_types',
        return_trans_column_type=True)
    print(X_deconfounded.head())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

            age  sex__:type:__continuous       bmi  ...        s4        s5        s6
    0  0.029271                 0.050680  0.057228  ... -0.019424  0.012311 -0.028194
    1  0.005874                -0.044642 -0.047538  ... -0.024667 -0.061637 -0.082913
    2  0.076494                 0.050680  0.039983  ... -0.019424 -0.004734 -0.036479
    3 -0.081307                -0.044642 -0.007659  ...  0.049135  0.029385 -0.000071
    4  0.013139                -0.044642 -0.032449  ...  0.012234 -0.025299 -0.037349

    [5 rows x 10 columns]




Because the confound is treated as a normal continuous feature
after removal it will be transformed in the pca as well


.. code-block:: default

    X_transformed, _ = confound_as_feature_pipe.preprocess(df_features, target)
    print(X_transformed.head())




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

       pca_component:0  pca_component:1  ...  pca_component:8  pca_component:9
    0        -0.014050         0.075715  ...        -0.008604        -0.002330
    1        -0.099883        -0.062830  ...         0.024022         0.002074
    2        -0.029014         0.053253  ...        -0.001197        -0.002579
    3         0.035164        -0.001321  ...        -0.006567        -0.003546
    4        -0.003952        -0.025446  ...         0.002095        -0.000516

    [5 rows x 10 columns]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.569 seconds)


.. _sphx_glr_download_auto_examples_advanced_run_advanced_confound_removal.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_advanced_confound_removal.py <run_advanced_confound_removal.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_advanced_confound_removal.ipynb <run_advanced_confound_removal.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

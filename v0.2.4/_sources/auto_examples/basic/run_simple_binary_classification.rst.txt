.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_basic_run_simple_binary_classification.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_basic_run_simple_binary_classification.py:


Simple Binary Classification
============================

This example uses the 'iris' dataset and performs a simple binary
classification using a Support Vector Machine classifier.

.. include:: ../../links.inc


.. code-block:: default

    # Authors: Federico Raimondo <f.raimondo@fz-juelich.de>
    #
    # License: AGPL
    from seaborn import load_dataset
    from julearn import run_cross_validation
    from julearn.utils import configure_logging





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/travis/virtualenv/python3.7.1/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject
      return f(*args, **kwds)




Set the logging level to info to see extra information


.. code-block:: default

    configure_logging(level='INFO')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-12-09 16:25:55,466 - julearn - INFO - ===== Lib Versions =====
    2020-12-09 16:25:55,467 - julearn - INFO - numpy: 1.16.4
    2020-12-09 16:25:55,467 - julearn - INFO - scipy: 1.5.4
    2020-12-09 16:25:55,467 - julearn - INFO - sklearn: 0.23.2
    2020-12-09 16:25:55,467 - julearn - INFO - pandas: 1.1.5
    2020-12-09 16:25:55,467 - julearn - INFO - julearn: 0.2.5.dev1+g8d00017
    2020-12-09 16:25:55,467 - julearn - INFO - ========================





.. code-block:: default

    df_iris = load_dataset('iris')








The dataset has three kind of species. We will keep two to perform a binary
classification.


.. code-block:: default

    df_iris = df_iris[df_iris['species'].isin(['versicolor', 'virginica'])]








As features, we will use the sepal length, width and petal length.
We will try to predict the species.


.. code-block:: default


    X = ['sepal_length', 'sepal_width', 'petal_length']
    y = 'species'
    scores = run_cross_validation(
        X=X, y=y, data=df_iris, model='svm', preprocess_X='zscore')

    print(scores['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-12-09 16:25:55,472 - julearn - INFO - Using default CV
    2020-12-09 16:25:55,472 - julearn - INFO - ==== Input Data ====
    2020-12-09 16:25:55,472 - julearn - INFO - Using dataframe as input
    2020-12-09 16:25:55,472 - julearn - INFO - Features: ['sepal_length', 'sepal_width', 'petal_length']
    2020-12-09 16:25:55,472 - julearn - INFO - Target: species
    2020-12-09 16:25:55,472 - julearn - INFO - Expanded X: ['sepal_length', 'sepal_width', 'petal_length']
    2020-12-09 16:25:55,472 - julearn - INFO - Expanded Confounds: []
    2020-12-09 16:25:55,473 - julearn - INFO - ====================
    2020-12-09 16:25:55,473 - julearn - INFO - 
    2020-12-09 16:25:55,474 - julearn - INFO - ====== Model ======
    2020-12-09 16:25:55,474 - julearn - INFO - Obtaining model by name: svm
    2020-12-09 16:25:55,474 - julearn - INFO - ===================
    2020-12-09 16:25:55,474 - julearn - INFO - 
    2020-12-09 16:25:55,474 - julearn - INFO - CV interpreted as RepeatedKFold with 5 repetitions of 5 folds
    0     0.95
    1     0.90
    2     1.00
    3     0.85
    4     0.90
    5     1.00
    6     0.85
    7     0.90
    8     1.00
    9     0.85
    10    0.95
    11    0.95
    12    0.90
    13    0.90
    14    0.85
    15    0.85
    16    0.85
    17    1.00
    18    0.90
    19    0.95
    20    0.85
    21    0.90
    22    0.95
    23    0.95
    24    0.95
    Name: test_score, dtype: float64




Additionally, we can choose to assess the performance of the model using
different scoring functions.

For example, we might have an unbalanced dataset:


.. code-block:: default


    df_unbalanced = df_iris[20:]  # drop the first 20 versicolor samples
    print(df_unbalanced['species'].value_counts())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    virginica     50
    versicolor    30
    Name: species, dtype: int64




If we compute the `accuracy`, we might not account for this imbalance. A more
suitable metric is the `balanced_accuracy`. More information in scikit-learn:
`Balanced Accuracy`_

We will also set the random seed so we always split the data in the same way.


.. code-block:: default

    scores = run_cross_validation(
        X=X, y=y, data=df_unbalanced, model='svm', seed=42, preprocess_X='zscore',
        scoring=['accuracy', 'balanced_accuracy'])

    print(scores['test_accuracy'].mean())
    print(scores['test_balanced_accuracy'].mean())






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-12-09 16:25:55,976 - julearn - INFO - Setting random seed to 42
    2020-12-09 16:25:55,976 - julearn - INFO - Using default CV
    2020-12-09 16:25:55,976 - julearn - INFO - ==== Input Data ====
    2020-12-09 16:25:55,976 - julearn - INFO - Using dataframe as input
    2020-12-09 16:25:55,976 - julearn - INFO - Features: ['sepal_length', 'sepal_width', 'petal_length']
    2020-12-09 16:25:55,976 - julearn - INFO - Target: species
    2020-12-09 16:25:55,976 - julearn - INFO - Expanded X: ['sepal_length', 'sepal_width', 'petal_length']
    2020-12-09 16:25:55,976 - julearn - INFO - Expanded Confounds: []
    2020-12-09 16:25:55,977 - julearn - INFO - ====================
    2020-12-09 16:25:55,977 - julearn - INFO - 
    2020-12-09 16:25:55,977 - julearn - INFO - ====== Model ======
    2020-12-09 16:25:55,977 - julearn - INFO - Obtaining model by name: svm
    2020-12-09 16:25:55,977 - julearn - INFO - ===================
    2020-12-09 16:25:55,978 - julearn - INFO - 
    2020-12-09 16:25:55,978 - julearn - INFO - CV interpreted as RepeatedKFold with 5 repetitions of 5 folds
    0.895
    0.8708886668886668




Other kind of metrics allows us to evaluate how good our model is to detect
specific targets. Suppose we want to create a model that correctly identifies
the `versicolor` samples.

Now we might want to evaluate the precision score, or the ratio of true
positives (tp) over all positives (true and false positives). More
information in scikit-learn: `Precision`_

For this metric to work, we need to define which are our `positive` values.
In this example, we are interested in detecting `versicolor`.


.. code-block:: default

    precision_scores = run_cross_validation(
        X=X, y=y, data=df_unbalanced, model='svm', preprocess_X='zscore', seed=42,
        scoring='precision', pos_labels='versicolor')
    print(precision_scores['test_precision'].mean())




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    2020-12-09 16:25:56,668 - julearn - INFO - Setting random seed to 42
    2020-12-09 16:25:56,668 - julearn - INFO - Using default CV
    2020-12-09 16:25:56,668 - julearn - INFO - ==== Input Data ====
    2020-12-09 16:25:56,668 - julearn - INFO - Using dataframe as input
    2020-12-09 16:25:56,668 - julearn - INFO - Features: ['sepal_length', 'sepal_width', 'petal_length']
    2020-12-09 16:25:56,668 - julearn - INFO - Target: species
    2020-12-09 16:25:56,668 - julearn - INFO - Expanded X: ['sepal_length', 'sepal_width', 'petal_length']
    2020-12-09 16:25:56,668 - julearn - INFO - Expanded Confounds: []
    2020-12-09 16:25:56,669 - julearn - INFO - Setting the following as positive labels ['versicolor']
    2020-12-09 16:25:56,669 - julearn - INFO - ====================
    2020-12-09 16:25:56,669 - julearn - INFO - 
    2020-12-09 16:25:56,669 - julearn - INFO - ====== Model ======
    2020-12-09 16:25:56,669 - julearn - INFO - Obtaining model by name: svm
    2020-12-09 16:25:56,669 - julearn - INFO - ===================
    2020-12-09 16:25:56,669 - julearn - INFO - 
    2020-12-09 16:25:56,670 - julearn - INFO - CV interpreted as RepeatedKFold with 5 repetitions of 5 folds
    0.9223333333333333





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  1.738 seconds)


.. _sphx_glr_download_auto_examples_basic_run_simple_binary_classification.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: run_simple_binary_classification.py <run_simple_binary_classification.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: run_simple_binary_classification.ipynb <run_simple_binary_classification.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
